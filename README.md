**Code in this project is HEAVILY based on the examples provided by the GGML and llama.cpp repositories.**

# Goals of this project 
1. Port SEA-LION 7B and its custom tokenizer to the GGUF format 
2. Run inference on a quantized version of the model on my local machine 
3. Serve as an educational reference for people who want to learn more about the SEA-LION model and GGML 

# Why not work on a direct fork of llama.cpp? 
This project is still in the "hacking" phase, so I would prefer to work with as minimal of a code footprint as possible.

Changes will eventually make its way into a PR for llama.cpp. 


